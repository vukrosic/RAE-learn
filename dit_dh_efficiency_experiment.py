#!/usr/bin/env python3
"""
DiT DH Efficiency Experiment

Compares:
- Standard Wide DiT (width=2048 throughout, depth=28)
- DiT DH (body width=1152 depth=28 + head width=2048 depth=2)

Measures:
- Training speed (steps/sec)
- Memory usage
- Parameter count
- Final loss (convergence quality)
"""

import os
import torch
from torch.optim import AdamW
import time
import numpy as np
from tqdm import tqdm
import argparse
from stage1 import RAE
from stage2.models.lightningDiT import LightningDiT
from stage2.models.DDT import DiTwDDTHead
from stage2.transport import create_transport
from omegaconf import OmegaConf

def get_memory_usage():
    """Get current GPU memory usage in MB."""
    if torch.cuda.is_available():
        return torch.cuda.max_memory_allocated() / 1024**2
    return 0

def count_parameters(model):
    """Count trainable parameters."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def create_standard_dit(token_dim=768, width=2048, depth=28):
    """Create standard wide DiT."""
    num_heads = width // 64
    model = LightningDiT(
        input_size=16,
        patch_size=1,
        in_channels=token_dim,
        hidden_size=width,
        depth=depth,
        num_heads=num_heads,
        mlp_ratio=4.0,
        num_classes=1000,
        use_qknorm=False,
        use_swiglu=True,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False,
        use_gembed=True,
    )
    return model

def create_dit_dh(token_dim=768, body_width=1152, head_width=2048, body_depth=28, head_depth=2):
    """Create DiT with DDT Head."""
    body_heads = body_width // 64
    head_heads = head_width // 64
    
    model = DiTwDDTHead(
        input_size=16,
        patch_size=1,
        in_channels=token_dim,
        hidden_size=[body_width, head_width],
        depth=[body_depth, head_depth],
        num_heads=[body_heads, head_heads],
        mlp_ratio=4.0,
        num_classes=1000,
        use_qknorm=False,
        use_swiglu=True,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False,
    )
    return model

def benchmark_model(model, latent, num_steps, device):
    """Benchmark training speed and memory for a model."""
    model = model.to(device)
    model.train()
    
    optimizer = AdamW(model.parameters(), lr=1e-4)
    transport = create_transport(
        path_type='Linear',
        prediction='velocity',
        loss_weight='velocity',
        time_dist_type='uniform',
        time_dist_shift=1.0,
    )
    
    y = torch.tensor([0], device=device)
    
    # Warm up
    for _ in range(3):
        optimizer.zero_grad()
        loss_dict = transport.training_losses(model, latent, dict(y=y))
        loss = loss_dict["loss"].mean()
        loss.backward()
        optimizer.step()
    
    # Reset memory counter after warm-up
    torch.cuda.reset_peak_memory_stats()
    
    # Benchmark
    losses = []
    start_time = time.time()
    
    for step in range(num_steps):
        optimizer.zero_grad()
        loss_dict = transport.training_losses(model, latent, dict(y=y))
        loss = loss_dict["loss"].mean()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    
    elapsed = time.time() - start_time
    steps_per_sec = num_steps / elapsed
    max_memory = get_memory_usage()
    
    return {
        'losses': losses,
        'final_loss': losses[-1],
        'steps_per_sec': steps_per_sec,
        'max_memory_mb': max_memory,
        'params': count_parameters(model),
    }

def main(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}\n")
    
    # Load RAE and get a latent
    print("Loading RAE and creating test latent...")
    rae_config = OmegaConf.create({
        'target': 'stage1.RAE',
        'params': {
            'encoder_cls': 'Dinov2withNorm',
            'encoder_config_path': 'facebook/dinov2-with-registers-base',
            'encoder_input_size': 224,
            'encoder_params': {'dinov2_path': 'facebook/dinov2-with-registers-base', 'normalize': True},
            'decoder_config_path': 'configs/decoder/ViTXL',
            'pretrained_decoder_path': 'models/decoders/dinov2/wReg_base/ViTXL_n08/model.pt',
            'noise_tau': 0.0,
            'reshape_to_2d': True,
            'normalization_stat_path': 'models/stats/dinov2/wReg_base/imagenet1k/stat.pt',
        }
    })
    rae = RAE(**rae_config.params).to(device)
    rae.eval()
    
    # Create dummy latent
    with torch.no_grad():
        dummy_img = torch.randn(args.batch_size, 3, 256, 256).to(device)
        latent = rae.encode(dummy_img)
    
    token_dim = latent.shape[1]
    print(f"Latent shape: {latent.shape}, token_dim: {token_dim}\n")
    
    # Test Standard Wide DiT
    print("=" * 70)
    print("EXPERIMENT A: Standard DiT (width=1152, depth=28)")
    print("=" * 70)
    
    standard_dit = create_standard_dit(token_dim=token_dim, width=1152, depth=28)
    standard_params = count_parameters(standard_dit)
    print(f"Parameters: {standard_params:,}")
    
    print(f"Benchmarking for {args.num_steps} steps...")
    standard_results = benchmark_model(standard_dit, latent, args.num_steps, device)
    
    print(f"Final Loss: {standard_results['final_loss']:.4f}")
    print(f"Speed: {standard_results['steps_per_sec']:.2f} steps/sec")
    print(f"Peak Memory: {standard_results['max_memory_mb']:.1f} MB\n")
    
    # Clean up
    del standard_dit
    torch.cuda.empty_cache()
    time.sleep(1)
    
    # Test DiT DH
    print("=" * 70)
    print("EXPERIMENT B: DiT DH (body=768 d28 + head=1152 d2)")
    print("=" * 70)
    
    dit_dh = create_dit_dh(
        token_dim=token_dim, 
        body_width=768, head_width=1152,
        body_depth=28, head_depth=2
    )
    dh_params = count_parameters(dit_dh)
    print(f"Parameters: {dh_params:,}")
    
    print(f"Benchmarking for {args.num_steps} steps...")
    dh_results = benchmark_model(dit_dh, latent, args.num_steps, device)
    
    print(f"Final Loss: {dh_results['final_loss']:.4f}")
    print(f"Speed: {dh_results['steps_per_sec']:.2f} steps/sec")
    print(f"Peak Memory: {dh_results['max_memory_mb']:.1f} MB\n")
    
    # Summary
    print("=" * 70)
    print("COMPARISON SUMMARY")
    print("=" * 70)
    
    param_reduction = (1 - dh_params / standard_params) * 100
    speed_improvement = ((dh_results['steps_per_sec'] / standard_results['steps_per_sec']) - 1) * 100
    memory_reduction = (1 - dh_results['max_memory_mb'] / standard_results['max_memory_mb']) * 100
    loss_diff = dh_results['final_loss'] - standard_results['final_loss']
    
    print(f"\n| Metric | Standard DiT | DiT DH | Improvement |")
    print(f"|--------|-------------|--------|-------------|")
    print(f"| Parameters | {standard_params:,} | {dh_params:,} | {param_reduction:+.1f}% ‚úÖ |")
    print(f"| Speed (steps/sec) | {standard_results['steps_per_sec']:.2f} | {dh_results['steps_per_sec']:.2f} | {speed_improvement:+.1f}% {'‚úÖ' if speed_improvement > 0 else '‚ö†Ô∏è'} |")
    print(f"| Memory (MB) | {standard_results['max_memory_mb']:.0f} | {dh_results['max_memory_mb']:.0f} | {memory_reduction:+.1f}% ‚úÖ |")
    print(f"| Final Loss | {standard_results['final_loss']:.4f} | {dh_results['final_loss']:.4f} | {loss_diff:+.4f} {'‚úÖ' if abs(loss_diff) < 0.05 else '‚ö†Ô∏è'} |")
    
    print(f"\nüéØ Key Takeaway:")
    print(f"DiT DH achieves {param_reduction:.0f}% parameter reduction and {memory_reduction:.0f}% memory")
    print(f"savings while maintaining comparable loss ({abs(loss_diff):.4f} difference).")
    print(f"Training speed: {speed_improvement:+.1f}% change.\n")
    
    # Save results
    os.makedirs(args.output_dir, exist_ok=True)
    with open(os.path.join(args.output_dir, 'efficiency_summary.txt'), 'w') as f:
        f.write(f"DiT DH Efficiency Comparison\n")
        f.write(f"{'='*50}\n\n")
        f.write(f"Standard DiT (width=1152, depth=28):\n")
        f.write(f"  Parameters: {standard_params:,}\n")
        f.write(f"  Speed: {standard_results['steps_per_sec']:.2f} steps/sec\n")
        f.write(f"  Memory: {standard_results['max_memory_mb']:.0f} MB\n")
        f.write(f"  Final Loss: {standard_results['final_loss']:.4f}\n\n")
        f.write(f"DiT DH (body=1152 d28, head=2048 d2):\n")
        f.write(f"  Parameters: {dh_params:,}\n")
        f.write(f"  Speed: {dh_results['steps_per_sec']:.2f} steps/sec\n")
        f.write(f"  Memory: {dh_results['max_memory_mb']:.0f} MB\n")
        f.write(f"  Final Loss: {dh_results['final_loss']:.4f}\n\n")
        f.write(f"Improvements:\n")
        f.write(f"  Parameters: {param_reduction:+.1f}%\n")
        f.write(f"  Speed: {speed_improvement:+.1f}%\n")
        f.write(f"  Memory: {memory_reduction:+.1f}%\n")
        f.write(f"  Loss difference: {loss_diff:+.4f}\n")
    
    print(f"‚úÖ Results saved to {args.output_dir}/efficiency_summary.txt")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch-size', type=int, default=4, help='Batch size')
    parser.add_argument('--num-steps', type=int, default=50, help='Number of training steps for benchmark')
    parser.add_argument('--output-dir', type=str, default='dit_dh_results', help='Output directory')
    args = parser.parse_args()
    main(args)

